{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Project:  Allstate Claims Severity\n",
    "#### Author:   Joshep Downs, James Peng, Megan Pera, Diana Rodenberger \n",
    "#### Purpose:  Predicting cost and severity of claims for AllState\n",
    "#### Created:  12/6/2016 \n",
    "#### Submitted: 12/6/2016 \n",
    "\n",
    "### Team name in Kaggle: UCB_207_1\n",
    "\n",
    "## Link to Leaderboard\n",
    "https://www.kaggle.com/c/allstate-claims-severity/leaderboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import unittest\n",
    "\n",
    "# General libraries.\n",
    "import re, os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import compress\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Ridge\n",
    "from datetime import datetime\n",
    "from sklearn.feature_selection import mutual_info_regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the datasets that Diana created and tries to pare down the number of variables by selecting relevant variables using mutual_info_regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and split train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"~/Downloads/AllstateChallenge-master/data_out/X_dummies_train.csv\")\n",
    "y = pd.read_csv(\"~/Downloads/AllstateChallenge-master/data_out/y_train.csv\", header=None)\n",
    "id = pd.read_csv(\"~/Downloads/AllstateChallenge-master/data_out/id_train.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set variables to hold dev and training data\n",
    "dev_data, dev_labels, dev_id = X[168318:], y[168318:], id[168318:]\n",
    "train_data, train_labels, train_id = X[:168318], y[:168318], id[:168318]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168318, 1190)\n",
      "(168318, 1)\n",
      "(20000, 1190)\n",
      "(20000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(dev_data.shape)\n",
    "print(dev_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a baseline Ridge regression for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_absolute_error on test data 0.1900172851793714\n"
     ]
    }
   ],
   "source": [
    "# Run a Ridge regression and evaluate MAE\n",
    "lr1 = linear_model.Ridge(alpha=0.00001, normalize=True)\n",
    "lr1.fit(train_data, train_labels)\n",
    "\n",
    "#use same linear model previously fit with training data\n",
    "dev_log_pred = lr1.predict(dev_data)\n",
    "\n",
    "mae = mean_absolute_error(dev_labels, dev_log_pred)\n",
    "print('mean_absolute_error on test data {0}'.format(mae))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with mutual info regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full = train_data\n",
    "full['loss'] = train_labels\n",
    "mini = full.sample(1000, random_state=1)\n",
    "\n",
    "# Pull out target vector\n",
    "loss_array = np.asarray(mini.loc[:,('loss')]) \n",
    "\n",
    "# Prepare mini matrix\n",
    "mini.drop('loss', axis=1, inplace=True)\n",
    "col = list(mini.columns)\n",
    "mini_train_matrix = mini.as_matrix(col) # variable df, as a matrix\n",
    "\n",
    "# Running mutual_info_regression\n",
    "feature_info = mutual_info_regression(mini_train_matrix,loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns an array of estimated mutual information between each feature and the loss target\n",
    "# the kernel dies when you do the whole dataset; not ideal\n",
    "# Writing a function that runs mutual_info_regression on a random chunk of data at a time\n",
    "\n",
    "# Pulling loss back into the data set for random sampling purposes\n",
    "full = train_data\n",
    "full['loss'] = train_labels\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "def find_features(n):\n",
    "    '''\n",
    "    This function returns a list of features to keep in the data set for regression.\n",
    "    Randomly samples and calculates mutual_info_regression n times. \n",
    "    '''\n",
    "    i = 0\n",
    "    features = []\n",
    "    \n",
    "    while i < n:\n",
    "        # Pulling a random chunk of data from X \n",
    "        full = train_data\n",
    "        full['loss'] = train_labels\n",
    "        mini = full.sample(1000)\n",
    "        \n",
    "        # Pull out target vector\n",
    "        loss_array = np.asarray(mini.loc[:,('loss')]) \n",
    "        \n",
    "        # Prepare mini matrix\n",
    "        mini.drop('loss', axis=1, inplace=True)\n",
    "        col = list(mini.columns)\n",
    "        mini_train_matrix = mini.as_matrix(col) # variable df, as a matrix\n",
    "\n",
    "        # Running mutual_info_regression\n",
    "        feature_info = mutual_info_regression(mini_train_matrix,loss_array)\n",
    "        \n",
    "        # Finding features that return more than 0 information \n",
    "        keep = feature_info > 0\n",
    "        \n",
    "        if i == 0:\n",
    "            features = list(compress(col, keep)) \n",
    "        else:\n",
    "            features2 = list(compress(col, keep))\n",
    "            features = set(features).intersection(features2)\n",
    "        i += 1\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This took 92.34055280685425 seconds\n"
     ]
    }
   ],
   "source": [
    "# Running 5 times and timing the process\n",
    "t1 = datetime.today().timestamp() # start timer\n",
    "keep = find_features(5)\n",
    "t2 = datetime.today().timestamp() # end timer\n",
    "print(\"This took\",t2-t1,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual info regression cuts down the number of variables from 1191 to 613\n"
     ]
    }
   ],
   "source": [
    "# Pull out variable names that contribute information\n",
    "names = []\n",
    "for item in keep:\n",
    "    delim_pos=int(np.core.defchararray.find(item,'_'))\n",
    "    var=item[:delim_pos]\n",
    "    names.append(var)\n",
    "vars = np.unique(np.asarray(names))\n",
    "\n",
    "# Include all categories of relevant variables\n",
    "col = list(train_data.columns)\n",
    "final = []\n",
    "for i in range(0,len(col)):\n",
    "    for v in vars:\n",
    "        if col[i][:delim_pos] == v:\n",
    "            final.append(col[i])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "print(\"Mutual info regression cuts down the number of variables from\",len(col), \"to\", len(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 613)\n",
      "(168318, 613)\n"
     ]
    }
   ],
   "source": [
    "# pare down data sets to only use the 885 relevant variables\n",
    "dev_set = dev_data[final]\n",
    "train_set = train_data[final]\n",
    "\n",
    "print(dev_set.shape)\n",
    "print(train_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a Ridge regression with optimized data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_absolute_error on test data 0.21792750669700212\n"
     ]
    }
   ],
   "source": [
    "# Run a Ridge regression and evaluate MAE\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "lr2 = linear_model.Ridge(alpha=0.00001, normalize=True)\n",
    "lr2.fit(train_set, train_labels)\n",
    "\n",
    "#use same linear model previously fit with training data\n",
    "dev_log_pred = lr2.predict(dev_set)\n",
    "\n",
    "mae = mean_absolute_error(dev_labels, dev_log_pred)\n",
    "print('mean_absolute_error on test data {0}'.format(mae))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In conclusion, using mutual_info_regression takes a long time and does not improve the mae above what is gained using L2 regression. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
