{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Project:  Allstate Claims Severity\n",
    "#### Author:   Joseph Downs, James Peng, Megan Pera, Diana Rodenberger \n",
    "#### Purpose:  Evaluate multiple regression models to predict cost and severity of claims for AllState\n",
    "#### Created:  10/29/2016\n",
    "\n",
    "### Team name in Kaggle: UCB_207_1\n",
    "\n",
    "## Link to Leaderboard\n",
    "https://www.kaggle.com/c/allstate-claims-severity/leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics included\n",
    "\n",
    "#### 1) Definition of global variables\n",
    "#### 2) Preprocessing train set: create dummy variables, split dataset into train and dev\n",
    "#### 3) Function to estimate best parameters for classifiers\n",
    "#### 4) Data Exploration of Continous Variables\n",
    "#### 5) Model 1: Linear Regression with dummy variables using RIDGE\n",
    "#### 6) Model 2: PCA and Regression\n",
    "#### 7) Model 3: Run regression models with Lasso\n",
    "#### 8) Model 4: Run regression models with ElasticNet\n",
    "#### 9) Model 5: Run regression models with DecisionTreeRegressor\n",
    "#### 10) Model 6: Run regression models with XGBoost\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import unittest\n",
    "\n",
    "# General libraries.\n",
    "import re, os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.feature_extraction import \n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# regularization methos\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##GLOBAL VARIABLES\n",
    "\n",
    "DUMMY_COLS=[]  #list of dummy variables\n",
    "CONTINOUS_COLS=[]  #list of continous variables\n",
    "#R_CLS          #Regression classifier defined in multiple sections of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing train set\n",
    "\n",
    " 1) Extract data and load into dataframe\n",
    " \n",
    " 2) Create dummy variables for categorical variables\n",
    " \n",
    " 3) Transform the variable 'loss'\n",
    " \n",
    " 4) Create training datasets and development datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125546, 131)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv('./data_in/train.csv')\n",
    "df_data.shape\n",
    "\n",
    "df_test = pd.read_csv('./data_in/test.csv')\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = df_data.columns\n",
    "cat_cols = [feat for feat in features if 'cat' in feat]\n",
    "cont_cols = [feat for feat in features if 'cont' in feat]\n",
    "\n",
    "#convert categorical variables into dummy variables\n",
    "df_dummy_vars=pd.get_dummies(\n",
    "                pd.concat([df_data[cat_cols], df_test[cat_cols]], axis=0)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_dummy_vars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_dummy_vars[:df_data.shape[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split data frame into categorical variables, continous variables (including all 'contx' \n",
    "#variables and 'loss') and ids\n",
    "\n",
    "features = df_data.columns\n",
    "cat_cols = [feat for feat in features if 'cat' in feat]\n",
    "cont_cols = [feat for feat in features if 'cont' in feat]\n",
    "\n",
    "\n",
    "df_cont_vars =  df_data[cont_cols]\n",
    "        \n",
    "    \n",
    "#convert categorical variables into dummy variables\n",
    "df_dummy_vars=pd.get_dummies(\n",
    "                pd.concat([df_data[cat_cols], df_test[cat_cols]], axis=0)\n",
    "                )\n",
    "\n",
    "#set global vars for later use\n",
    "DUMMY_COLS=df_dummy_vars.columns\n",
    "CONTINOUS_COLS=cont_cols\n",
    "\n",
    "\n",
    "#create dataset with predictors\n",
    "X = pd.concat([df_dummy_vars[:df_data.shape[0]], df_cont_vars], axis=1)\n",
    "#create dataset with dependent variable\n",
    "y = np.log10(df_data.loss)\n",
    "#create additioanl dataset with claim ids\n",
    "Id = df_data.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev, id_train, id_dev = train_test_split( X, y, Id, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (126173, 1190)\n",
      "y_train shape:  (126173,)\n",
      "X_dev shape:  (62145, 1190)\n",
      "y_dev shape:  (62145,)\n",
      "X_train shape:  (40000, 1190)\n",
      "y_train shape:  (40000,)\n"
     ]
    }
   ],
   "source": [
    "# Set variables to hold dev and training data\n",
    "# X_dev, y_dev, dev_id = X[168318:], y[168318:], id[168318:]\n",
    "# X_train, y_train, train_id = X[:168318], y[:168318], id[:168318]\n",
    "X_train_mini, y_train_mini, id_train_mini = X_train[:40000], y_train[:40000], id_train[:40000]\n",
    "\n",
    "print('X_train shape: ',X_train.shape)\n",
    "print('y_train shape: ',y_train.shape)\n",
    "print('X_dev shape: ',X_dev.shape)\n",
    "print('y_dev shape: ',y_dev.shape)\n",
    "print('X_train shape: ',X_train_mini.shape)\n",
    "print('y_train shape: ',y_train_mini.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1_A</th>\n",
       "      <th>cat1_B</th>\n",
       "      <th>cat2_A</th>\n",
       "      <th>cat2_B</th>\n",
       "      <th>cat3_A</th>\n",
       "      <th>cat3_B</th>\n",
       "      <th>cat4_A</th>\n",
       "      <th>cat4_B</th>\n",
       "      <th>cat5_A</th>\n",
       "      <th>cat5_B</th>\n",
       "      <th>...</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18739</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380560</td>\n",
       "      <td>0.608838</td>\n",
       "      <td>0.546949</td>\n",
       "      <td>0.45289</td>\n",
       "      <td>0.42289</td>\n",
       "      <td>0.55533</td>\n",
       "      <td>0.511698</td>\n",
       "      <td>0.557380</td>\n",
       "      <td>0.642600</td>\n",
       "      <td>0.221177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155434</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568839</td>\n",
       "      <td>0.439206</td>\n",
       "      <td>0.407462</td>\n",
       "      <td>0.45883</td>\n",
       "      <td>0.46853</td>\n",
       "      <td>0.52221</td>\n",
       "      <td>0.441763</td>\n",
       "      <td>0.443374</td>\n",
       "      <td>0.324464</td>\n",
       "      <td>0.340194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 1190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat1_A  cat1_B  cat2_A  cat2_B  cat3_A  cat3_B  cat4_A  cat4_B  \\\n",
       "18739        1       0       1       0       1       0       1       0   \n",
       "155434       1       0       1       0       1       0       1       0   \n",
       "\n",
       "        cat5_A  cat5_B    ...        cont5     cont6     cont7    cont8  \\\n",
       "18739        0       1    ...     0.380560  0.608838  0.546949  0.45289   \n",
       "155434       0       1    ...     0.568839  0.439206  0.407462  0.45883   \n",
       "\n",
       "          cont9   cont10    cont11    cont12    cont13    cont14  \n",
       "18739   0.42289  0.55533  0.511698  0.557380  0.642600  0.221177  \n",
       "155434  0.46853  0.52221  0.441763  0.443374  0.324464  0.340194  \n",
       "\n",
       "[2 rows x 1190 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing greatest and lowest (bottom and top 10%) risk in a 2D feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize PCA, k = 2\n",
    "pca2 = PCA(n_components = 2)\n",
    "# Fit model and apply the dimensionality reduction to train_data\n",
    "values = pca2.fit_transform(X_train) # returns a (126173, 2) array\n",
    "\n",
    "# Pull out values that are high and low risk \n",
    "h,l =[],[] # initialize empty list\n",
    "risk = np.asarray(y_train)\n",
    "\n",
    "for k in range(0,126173): \n",
    "    if risk[k] >= 3.806554: \n",
    "        h.append(k) # append index to h if in top 10% of risk\n",
    "    elif risk[k] <= 2.888932:\n",
    "        l.append(k) # append index to l if in bottom 10% of risk\n",
    "\n",
    "# Split into high (above mean) and low (below mean) categories\n",
    "high = np.asarray([values[i] for i in h]) \n",
    "low = np.asarray([values[i] for i in l])\n",
    "h_toplot = np.transpose(high)\n",
    "l_toplot = np.transpose(low)\n",
    "\n",
    "# Plot arrays, red is high and blue is low\n",
    "fig = plt.subplots(figsize=(10.0,8.0))\n",
    "high, = plt.plot(h_toplot[0],h_toplot[1],'ro', label='High')\n",
    "low, = plt.plot(l_toplot[0],l_toplot[1],'bo',label='Low')\n",
    "plt.legend(handles=[high, low])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('2D Feature Space for Allstate Risk Classification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data Exploration of Continous Variables\n",
    "\n",
    "1) Check the distributions \n",
    "\n",
    "2) Check for autocorrelation between variables\n",
    "\n",
    "3) Check variances to see if noramalization is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#function to plot histograms of continous variables\n",
    "def PlotHisContinousVars(df):\n",
    "\n",
    "    plt.subplots(5,3, figsize=(10,15))\n",
    "    jj = 0\n",
    "    for column_name in df.columns:\n",
    "        jj+=1\n",
    "        plt.subplot(5,3,jj).hist(df[column_name],normed=False, bins=60)\n",
    "        plt.subplot(5,3,jj).set(title=column_name)\n",
    "    plt.show()\n",
    " \n",
    "PlotHisContinousVars(df_cont_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.cla()\n",
    "plt.subplot(1,1,1).hist(y_train,normed=True, bins=60)\n",
    "plt.subplot(1,1,1).set(title='loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing a scatter plot matrix for continuous variables to investigate autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building the scatterplot matrix using the training data\n",
    "scatter_matrix(df_cont_vars, alpha=0.02, figsize=(14, 14), diagonal='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the variance of continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below shows the variances for each of the 14 continuous variables in this dataset. All of the variances are less than 0.05, and the range of the variance is low (less than 0.02). This indicates that these variables have likely already been normalized. Since there is no great differences in variance between these variables, we will do no normalization ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Range of variance:',df_cont_vars.var().max() - df_cont_vars.var().min())\n",
    "df_cont_vars.var().plot(kind='bar', alpha =0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to estimate best parameters for classifiers\n",
    "\n",
    "Warning: The call to this function can take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def EvaluateHyperParams(cls, params, X_data, y_data):\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "     ('clf', cls),\n",
    "    ])\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, params, scoring = 'neg_mean_absolute_error')\n",
    "    grid_search.fit(X_data, y_data)\n",
    "    \n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    return best_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the baseline scores submitted for the competition\n",
    "\n",
    "mean_absolute_error on training data: [-0.19872471 -0.19753498 -0.19821047]\n",
    "\n",
    "mean_absolute_error on test data 0.198086964004104\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to estimate MAE for specified regression classifier (Ridge, Lasso, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EstimateMAEForRegression(cls, X_train,y_train,X_test, y_test):\n",
    "    \n",
    "    # Train the model using the training sets\n",
    "    cls.fit(X_train, y_train)\n",
    "\n",
    "    # predict dev set\n",
    "    preds = cls.predict(X_test)\n",
    "   \n",
    "    s = cross_val_score(cls, X_train, y_train, scoring='neg_mean_absolute_error')\n",
    "    \n",
    "    print('mean_absolute_error on training data: {0}'.format(s))\n",
    "\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    print('mean_absolute_error on dev data {0}'.format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Linear Regression with dummy variables using RIDGE\n",
    "\n",
    "We will try 3 different values of alpha\n",
    "\n",
    "TODO: Find optimal values of alpha combined with other hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find optimal value of alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'clf__alpha': [0.000001, 0.001, 0.01, 0.1 ],\n",
    "    'clf__normalize': [True]\n",
    "    }\n",
    "best_parameters=EvaluateHyperParams(cls=linear_model.Ridge(),params=parameters,X_data=X_train, y_data=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alpha =0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression - Ridge with alpha :  1e-05\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "alpha=0.00001\n",
    "\n",
    "r_cls=linear_model.Ridge(alpha=alpha, normalize=True)\n",
    "\n",
    "print(\"Regression - Ridge with alpha : \", alpha)\n",
    "\n",
    "EstimateMAEForRegression(cls=r_cls,X_train=X_train,y_train=y_train,X_test=X_dev, y_test=y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alpha =0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha=0.001\n",
    "\n",
    "r_cls=linear_model.Ridge(alpha=alpha, normalize=True)\n",
    "\n",
    "print(\"Regression - Ridge with alpha : \", alpha)\n",
    "\n",
    "EstimateMAEForRegression(cls=r_cls,X_train=X_train,y_train=y_train,X_test=X_dev, y_test=y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alpha =0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha=0.1\n",
    "\n",
    "r_cls=linear_model.Ridge(alpha=alpha, normalize=True)\n",
    "\n",
    "print(\"Regression - Ridge with alpha : \", alpha)\n",
    "\n",
    "EstimateMAEForRegression(cls=r_cls,X_train=X_train,y_train=y_train,X_test=X_dev, y_test=y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model 2: PCA and Regression\n",
    "\n",
    "Create a model for regression that uses PCA components for dummy variables + continous variables. Here are the steps to run this model:\n",
    "\n",
    "1) Create a dataset with PCA components of dummy variables. \n",
    "\n",
    "2) Create a dataset with just continous variables. \n",
    "\n",
    "3) Create a combined dataset using datasets from step 1 and 2.\n",
    "\n",
    "4) Do steps 1-3 for training data and dev data.\n",
    "\n",
    "5) Run regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing: Find the number of components that explains most of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ExploreNumPCAComponents(n_pca_comp):\n",
    "\n",
    "    pca = PCA(n_components=n_pca_comp)\n",
    "    pca.fit(X_train)\n",
    "    cum_vars=[]\n",
    "    components=[]\n",
    "    cumulative_var=0\n",
    "    for ii in range(len(pca.explained_variance_ratio_)):\n",
    "        cumulative_var+=pca.explained_variance_ratio_[ii]\n",
    "        cum_vars.append(cumulative_var)\n",
    "        components.append(ii+1)\n",
    "        \n",
    "    plt.plot(components, cum_vars)\n",
    "    \n",
    "    plt.show()\n",
    "    return cum_vars\n",
    "\n",
    "cum_vars=ExploreNumPCAComponents(200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Total variance explained by first PCA component: %0.6f \\n' % cum_vars[0])\n",
    "print('Total variance explained by 8 PCA components: %0.6f \\n' % cum_vars[8])\n",
    "print('Total variance explained by 50 PCA components: %0.6f \\n' % cum_vars[49])\n",
    "print('Total variance explained by 100 PCA components: %0.6f \\n' % cum_vars[99])\n",
    "print('Total variance explained by 215 PCA components: %0.6f \\n' % cum_vars[214])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total variance explained by 215 components is 95.93%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training set and Development Set with PCA components + Continous Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create a dataframe with PCA components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PCAComponentsToDataFrame(PCAComp, index_list):\n",
    "    #### Purpose: Transform PCA components to DataFrame\n",
    "    dic_comp={}\n",
    "    ii=0\n",
    "    for ii in range(PCAComp.shape[1]):\n",
    "        key= 'comp' + str(ii+1)\n",
    "        dic_comp[key]=PCAComp[:,ii]\n",
    "\n",
    "    #create dataframes with pca components\n",
    "    return pd.DataFrame(dic_comp, index=index_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a new training set and dev set with pca components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try 215 components because that explains the most variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_pca_comp=215\n",
    "\n",
    "#Run PCA with with 215 components\n",
    "pca = PCA(n_components=n_pca_comp)\n",
    "train_data_pca = pca.fit_transform(X_train)\n",
    "dev_data_pca = pca.transform(X_dev)\n",
    "\n",
    "#### Transform PCA components to DataFrame, preserve the index of the original datasets.\n",
    "X_train_pca=PCAComponentsToDataFrame(train_data_pca,list(id_train.index))\n",
    "X_dev_pca=PCAComponentsToDataFrame(dev_data_pca,list(id_dev.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#checks\n",
    "print(X_train.shape)\n",
    "print(X_train_pca.shape)\n",
    "print(X_dev_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run regression models with 215 PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'clf__alpha': (0.00001, .001, 0.1, 0.8),\n",
    "    'clf__solver': ('auto', 'sag'),\n",
    "    }\n",
    "best_parameters=EvaluateHyperParams(cls=linear_model.Ridge(),params=parameters,X_data=X_train_pca, y_data=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha=0.00001\n",
    "\n",
    "r_cls=linear_model.Ridge(alpha=alpha, normalize=True)\n",
    "\n",
    "print(\"Regression with PCA components and continous variables -Ridge with alpha : \", alpha)\n",
    "\n",
    "EstimateMAEForRegression(cls=r_cls,X_train=X_train_pca,y_train=y_train,X_test=X_dev_pca, y_test=y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha=0.1\n",
    "\n",
    "r_cls=linear_model.Ridge(alpha=alpha, normalize=True)\n",
    "\n",
    "print(\"Regression with PCA components and continous variables -Ridge with alpha : \", alpha)\n",
    "\n",
    "EstimateMAEForRegression(cls=r_cls,X_train=X_train_pca,y_train=y_train,X_test=X_dev_pca, y_test=y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha=0.0000001\n",
    "\n",
    "r_cls=linear_model.Ridge(alpha=alpha, normalize=True)\n",
    "\n",
    "print(\"Regression with PCA components and continous variables -Ridge with alpha : \", alpha)\n",
    "\n",
    "EstimateMAEForRegression(cls=r_cls,X_train=X_train_pca,y_train=y_train,X_test=X_dev_pca, y_test=y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function features columns with the larges coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SelectFeaturesLargestCoeff(cls, X):\n",
    "    #select columns with top coe\n",
    "    col_names=X.columns\n",
    "\n",
    "    sorted_index=np.argsort(cls.coef_)\n",
    "    best_coeff_vars=[]\n",
    "    for ii in range(15):\n",
    "        best_coeff_vars.append(col_names[sorted_index[(len(sorted_index)-ii-1)]])\n",
    "    return best_coeff_vars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use features with the largest coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_coeff_vars=[]\n",
    "best_coeff_vars=SelectFeaturesLargestCoeff(r_cls,X_train_pca)\n",
    "df_X_train_pca_best=X_train_pca[best_coeff_vars]\n",
    "df_X_dev_pca_best=X_dev_pca[best_coeff_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'clf__alpha': (0.00001, .001, 0.1, 0.8),\n",
    "     }\n",
    "best_parameters=EvaluateHyperParams(cls=linear_model.Ridge(),params=parameters,X_data=df_X_train_pca_best, y_data=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha=0.8\n",
    "\n",
    "r_cls=linear_model.Ridge(alpha=alpha, normalize=True)\n",
    "\n",
    "print(\"Regression using features with largest coefficients (PCA comp + cont var) -Ridge with alpha : \", alpha)\n",
    "\n",
    "EstimateMAEForRegression(cls=r_cls,X_train=df_X_train_pca_best,y_train=y_train,X_test=df_X_dev_pca_best, y_test=y_dev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Run regression models with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EvaluateHyperParams(cls, params, X_data, y_data):    \n",
    "    pipeline = Pipeline([\n",
    "     ('clf', cls),\n",
    "    ])\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, params)\n",
    "    grid_search.fit(X_data, y_data)\n",
    "    \n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    return best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'clf__alpha': (0.00001, .001, 0.1, 0.8),\n",
    "     }\n",
    "\n",
    "best_parameters=EvaluateHyperParams(cls=linear_model.Lasso(),params=parameters,X_data=X_train_mini, y_data=y_train_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha=0.00001\n",
    "\n",
    "r_cls=linear_model.Lasso(alpha=alpha, normalize=True)\n",
    "\n",
    "print(\"Regression using PCA comp + cont var -Lasso with alpha : \", alpha)\n",
    "\n",
    "EstimateMAEForRegression(cls=r_cls,X_train=X_train_mini,y_train=y_train_mini,X_test=X_dev, y_test=y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Run regression models with ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'clf__alpha': (0.00001, .001, 0.1, 0.8),\n",
    "     }\n",
    "\n",
    "best_parameters=EvaluateHyperParams(cls=linear_model.ElasticNet(),params=parameters,X_data=X_train_pca, y_data=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha=0.1\n",
    "\n",
    "r_cls=linear_model.ElasticNet(alpha=alpha, normalize=True)\n",
    "\n",
    "print(\"Regression -ElasticNet with alpha : \", alpha)\n",
    "\n",
    "EstimateMAEForRegression(cls=r_cls,X_train=X_train,y_train=y_train,X_test=X_dev, y_test=y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Run regression models with DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_depth=5\n",
    "\n",
    "r_cls=DecisionTreeRegressor(max_depth=max_depth)\n",
    "\n",
    "print(\"Regression -DecisionTreeRegressor with max_depth : \", max_depth)\n",
    "\n",
    "EstimateMAEForRegression(cls=r_cls,X_train=X_train,y_train=y_train,X_test=X_dev, y_test=y_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_depth=15\n",
    "\n",
    "r_cls=DecisionTreeRegressor(max_depth=max_depth)\n",
    "\n",
    "print(\"Regression -DecisionTreeRegressor with max_depth : \", max_depth)\n",
    "\n",
    "EstimateMAEForRegression(cls=r_cls,X_train=X_train,y_train=y_train,X_test=X_dev, y_test=y_dev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import xgboost python module\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fitting model on training data\n",
    "xgbr = xgb.XGBRegressor(max_depth=6, n_estimators=500, learning_rate=0.1, subsample=0.8, \n",
    "                        colsample_bytree=0.4, min_child_weight = 3,  seed=7)\n",
    "\n",
    "EstimateMAEForRegression(cls=xgbr,X_train=X_train,y_train=y_train,X_test=X_dev, y_test=y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: AdaBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "abr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=5))\n",
    "EstimateMAEForRegression(cls=abr,X_train=X_train_mini,y_train=y_train_mini,X_test=X_dev, y_test=y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1) Extract data and load into dataframe\n",
    " \n",
    " 2) Create dummy variables for categorical variables\n",
    " \n",
    " 3) Predict loss based on 'best' model previously found : Ridge with alpha= 0.00001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# = pd.read_csv('./data_in/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_cat_cols=[]\n",
    "#test_cont_cols=[]\n",
    "#for c in df_test.columns:\n",
    "    #if c.find('cat') >=0: # -1: substring not found, >=0, starting index\n",
    "        #test_cat_cols.append(c)\n",
    "    #elif c.find('cont')>=0:\n",
    "        #test_cont_cols.append(c)\n",
    "\n",
    "#test_df_cont_vars = df_test[test_cont_cols]\n",
    "        \n",
    "#convert categorical variables into dummy variables\n",
    "#test_df_dummy_vars=pd.get_dummies(df_test[test_cat_cols])\n",
    "\n",
    "#create dataset with predictors\n",
    "#X_test = pd.concat([test_df_dummy_vars, test_df_cont_vars], axis=1)\n",
    "#id_test = df_test.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#print('num columns in test: ', X_test.shape[1])\n",
    "#print('num columns in train: ', X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important note:\n",
    "\n",
    "When transforming categorical variables into dummy variables, we get a different set of variables for the training data and the test data. The train data and the test data have different categories for some categorical variables; thus, when we tranform categorical variables to dummy variables then a different set of dummy variables for the train and test data. \n",
    "\n",
    "See file 'Allstate - Create train and test datasets with dummy vars' to fix this problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
